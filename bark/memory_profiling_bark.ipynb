{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bark Memory Profiling\n",
    "Bark has two ways to reduce GPU memory: \n",
    " - Small models: a smaller version of the model. This can be set by using the environment variable `SUNO_USE_SMALL_MODELS`\n",
    " - offloading models to CPU: Holding only one model at a time on the GPU, and shuttling the models to the CPU in between generations. \n",
    "\n",
    "# $ \\\\ $\n",
    "## First, we'll use the most memory efficient configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"SUNO_USE_SMALL_MODELS\"] = \"1\"\n",
    "os.environ[\"SUNO_OFFLOAD_CPU\"] = \"1\"\n",
    "\n",
    "from bark.generation import (\n",
    "    generate_text_semantic,\n",
    "    preload_models,\n",
    ")\n",
    "from bark import generate_audio, SAMPLE_RATE\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU being used. Careful, inference might be very slow!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f201e0f696e4b7d8f26ae6924d10c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text.pt:   0%|          | 0.00/2.32G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb7bf74113f479893e2ca345a5dc12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d92906157647588339895cc074b5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ba58f4f33c4a64a2f18df2eaa2c0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ce197c7aaa44c988ed03f16472b141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99549ea18d744f8db894fea7471a88d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "coarse.pt:   0%|          | 0.00/1.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3397b7726e47b08c57ff513be776e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fine.pt:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imjisu/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/encodec/v0/encodec_24khz-d7cc33bc.th\" to /Users/imjisu/.cache/torch/hub/checkpoints/encodec_24khz-d7cc33bc.th\n",
      "100%|██████████████████████████████████████| 88.9M/88.9M [00:08<00:00, 11.0MB/s]\n",
      "100%|█████████████████████████████████████████| 192/192 [00:09<00:00, 20.16it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [01:01<00:00,  6.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max memory usage = 0MB\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.reset_peak_memory_stats()\n",
    "preload_models()\n",
    "audio_array = generate_audio(\"madam I'm adam\", history_prompt=\"v2/en_speaker_5\")\n",
    "max_utilization = torch.cuda.max_memory_allocated()\n",
    "print(f\"max memory usage = {max_utilization / 1024 / 1024:.0f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Profiling:\n",
    "We can profile the memory consumption of 4 scenarios\n",
    " - Small models, offloading to CPU\n",
    " - Large models, offloading to CPU\n",
    " - Small models, not offloading to CPU\n",
    " - Large models, not offloading to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from bark.generation import (\n",
    "    generate_text_semantic,\n",
    "    preload_models,\n",
    "    models,\n",
    ")\n",
    "import bark.generation\n",
    "\n",
    "from bark.api import semantic_to_waveform\n",
    "from bark import generate_audio, SAMPLE_RATE\n",
    "\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU being used. Careful, inference might be very slow!\n",
      "/Users/imjisu/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "No GPU being used. Careful, inference might be very slow!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small models True, offloading to CPU: True\n",
      "\tmax memory usage = 0MB, time 97s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU being used. Careful, inference might be very slow!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small models False, offloading to CPU: True\n",
      "\tmax memory usage = 0MB, time 110s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU being used. Careful, inference might be very slow!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small models True, offloading to CPU: False\n",
      "\tmax memory usage = 0MB, time 84s\n",
      "\n",
      "Small models False, offloading to CPU: False\n",
      "\tmax memory usage = 0MB, time 83s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global models\n",
    "\n",
    "for offload_models in (True, False):\n",
    "    # this setattr is needed to do on the fly\n",
    "    # the easier way to do this is with `os.environ[\"SUNO_OFFLOAD_CPU\"] = \"1\"`\n",
    "    setattr(bark.generation, \"OFFLOAD_CPU\", offload_models)\n",
    "    for use_small_models in (True, False):\n",
    "        models = {}\n",
    "        torch.cuda.empty_cache()\n",
    "#        torch.cuda.reset_peak_memory_stats()\n",
    "        preload_models(\n",
    "            text_use_small=use_small_models,\n",
    "            coarse_use_small=use_small_models,\n",
    "            fine_use_small=use_small_models,\n",
    "            force_reload=True,\n",
    "        )\n",
    "        t0 = time.time()\n",
    "        audio_array = generate_audio(\"madam I'm adam\", history_prompt=\"v2/en_speaker_5\", silent=True)\n",
    "        dur = time.time() - t0\n",
    "        max_utilization = torch.cuda.max_memory_allocated()\n",
    "        print(f\"Small models {use_small_models}, offloading to CPU: {offload_models}\")\n",
    "        print(f\"\\tmax memory usage = {max_utilization / 1024 / 1024:.0f}MB, time {dur:.0f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
