# Using My Own Music to Train and Sample

After experimenting with the pretrained models and better understanding the actual parameters for MusicVAE, I was able to begin working on finding and using my own data. As mentioned in the README in the parent directory, I was not able to locate specific datasets for music files correlated with EEG results. As such, I instead took music MIDIs qualitatively categorized as calming and soothing. Using these MIDIs, I explored how the MusicVAE framework could use this data to generate new music that had similar characteristics. My steps are listed below:

1. Select music MIDI samples from an online source (http://pooshouse.tripod.com/1music.html) and sort them into the "original_music" folder

2. Manually separate the music MIDI file tracks only have the chorus on track 1, using a MIDI editing software like MixPad.

3. Generate compatible 2-bar length MIDI segments using the corresponding command in ```COMMAND.md```.

4. Use the existing pretrained model from magenta to interpolate between these 2 bar MIDI segments using the corresponding command in ```COMMAND.md```.

5. Convert the compatible 2-bar MIDI files into NoteSequences that are stored in ```notesequencesupdated.tfrecord``` in order to be compatible with TensorFlow's framework. This can be done with a command detailed in ```COMMAND.md```.

6. Train my own VAE model using the MusicVAE framework on the compatible 2-bar MIDI segments using the corresponding command in ```COMMAND.md```.

7. Sample from my own VAE model using the corresponding command in ```COMMAND.md```.

Ultimately, this results in the training data of 2-bar MIDI sequences outputted in the "training_data" folder, the interpolated MIDI sequences in the "interpolate" folder, and the sampled results from my own model in the "results/samples" folder.

I also kept a number of MIDIs generated by sampling previous unsuccessful attempts that I had at training a VAE in the "results/failed" folder, as a record of my progress and development. As can be seen from the "results/failed" folder, my first model gave me a sample with only one note as the output. As I continued to iterate forward, I began getting more discordant notes being played together. Finally, as I was able to set the hyperparameters correctly, I achieved the MIDI files in the "results/sample" folder, which are far less discordant and more natural sounding.

The actual VAE model and the various training checkpoints could not be uploaded to GitHub due to the large file size, with each checkpoint being over 1.6 GB. However, as evidence of my creation of the model and for documentation purposes, I have uploaded the final checkpoint of my model as a .zip file to Google Drive, and linked the publicly shared .zip file here: https://drive.google.com/file/d/1WjrUTi98sQsxgYdQbhM4Zq8Ipay9Q4D2/view?usp=sharing

Note: The ```music_vae_train.py```, ```configs.py```, and ```base_model.py``` code files are directly from the magenta repository and are placed here for easy reference for the actual specifications and functionality of the model training process and the hyperparameters.
